{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Loading Custom Accentor\n",
            "============================================================\n",
            "‚úÖ Using CPU\n",
            "üìñ Loading vocabulary from C:\\Users\\–π—Ü—É–∫–µ–Ω\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ruaccent\\model\\vocab.json...\n",
            "   Vocabulary size: 224\n",
            "ü§ñ Initializing model...\n",
            "üì¶ Loading weights from C:\\Users\\–π—Ü—É–∫–µ–Ω\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ruaccent\\model\\acc_model.pt...\n",
            "‚úÖ Accentor initialized successfully!\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\–π—Ü—É–∫–µ–Ω\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2026-02-24 11:00:41,373 [INFO] pymorphy3.opencorpora_dict.wrapper - Loading dictionaries from C:\\Users\\–π—Ü—É–∫–µ–Ω\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pymorphy3_dicts_ru\\data\n",
            "2026-02-24 11:00:41,788 [INFO] pymorphy3.opencorpora_dict.wrapper - format: 2.4, revision: 417150, updated: 2022-01-08T22:09:24.565962\n"
          ]
        }
      ],
      "source": [
        "#–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from nltk import word_tokenize\n",
        "import json\n",
        "import telebot\n",
        "import heapq\n",
        "from telebot import TeleBot, types\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('russian')\n",
        "my_stop_words = {'–∫–∞', '–≤–µ—Å—å', '—Å–≤–æ–π', '—ç—Ç–æ', '–ª–∏—à—å'}\n",
        "stopwords = my_stop_words.union(stopwords)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from ruaccent import load_accentor\n",
        "accentor = load_accentor()\n",
        "from hybrid_vectorizer import HybridVectorizer\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nlp = spacy.load(\"ru_core_news_md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–ü–∞—Ä—Å–∏–Ω–≥ –∏–º—ë–Ω –∞–≤—Ç–æ—Ä–æ–≤, –Ω–∞–∑–≤–∞–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–æ–≤ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#–ø–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–∞–π—Ç–∞\n",
        "def page_generator (x,y):\n",
        "    page_links = []\n",
        "    for i in range(x, y):\n",
        "        page_links.append(f'https://www.culture.ru/literature/poems?page={i}')\n",
        "    return (page_links)\n",
        "\n",
        "#–ø–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è\n",
        "def link_generator (page_links):\n",
        "    poem_links = []\n",
        "    poem_elements = []\n",
        "    for i in page_links:\n",
        "        page = requests.get(i)\n",
        "        if page.status_code != 200:\n",
        "            print ('–æ—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Å—Ç—Ä–∞–Ω–∏—Ü–µ')\n",
        "            break\n",
        "        time.sleep(3)\n",
        "        soup = BeautifulSoup(page.text)\n",
        "        poem_elements = soup.find_all('a', class_='styles_PoemCard__Heading__SJxDJ')\n",
        "        for element in poem_elements:\n",
        "            href = element.get('href')\n",
        "            full_url = ['https://www.culture.ru' + href]\n",
        "            poem_links.extend(full_url)\n",
        "    return (poem_links)\n",
        "\n",
        "#—Å–æ–∑–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ\n",
        "def poem_finder (soup):\n",
        "    poem = soup.find (class_='styles_body__WEo9w')\n",
        "    poem = str(poem)\n",
        "    poem = poem.replace ('<br/>', '\\n')\n",
        "    poem = BeautifulSoup(poem, 'html.parser').text\n",
        "    return (poem)\n",
        "\n",
        "#—Å–æ–∑–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–º–µ–Ω–∏ –∞–≤—Ç–æ—Ä–∞\n",
        "def author_finder (soup):\n",
        "    author = soup.find (class_='styles_MainPoem__Author__SjIRK').text\n",
        "    return (author)\n",
        "\n",
        "#—Å–æ–∑–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç–∏—Ö–æ–≤—Ç–æ—Ä–µ–Ω–∏—è\n",
        "def title_finder (soup):\n",
        "    title = soup.find (class_='styles_MainPoem__Title__nvD6h').text\n",
        "    title = word_tokenize (title)\n",
        "    return (title)\n",
        "\n",
        "#–Ω–∞—Ö–æ–¥–∏–º –∏–º—è –∞–≤—Ç–æ—Ä–∞, –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ —Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–≥–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è\n",
        "author_list = []\n",
        "title_list = []\n",
        "poem_list = []\n",
        "ranges = [(1,5), (5,10), (10,15), (15,21)]\n",
        "for x, y in ranges:\n",
        "    page_links = page_generator (x,y)\n",
        "    print ('—Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:', page_links)\n",
        "    poem_links = link_generator (page_links)\n",
        "    print ('—Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è:', poem_links)\n",
        "    time.sleep(3)\n",
        "    for link in poem_links:\n",
        "        page = requests.get(link)\n",
        "        if page.status_code != 200:\n",
        "            print ('–æ—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Å—Ç—Ä–∞–Ω–∏—Ü–µ')\n",
        "            page = requests.get(link)\n",
        "            if page.status_code != 200:\n",
        "                print ('–ø–æ–≤—Ç–æ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Å—Ç—Ä–∞–Ω–∏—Ü–µ')\n",
        "                continue\n",
        "        time.sleep(3)\n",
        "        soup = BeautifulSoup(page.text)\n",
        "        author = author_finder (soup)\n",
        "        title = title_finder (soup)\n",
        "        poem = poem_finder (soup)\n",
        "        author_list.append (author)\n",
        "        title_list.append (title)\n",
        "        poem_list.append (poem)\n",
        "        print (title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ñ–∞–π–ª–µ\n",
        "#data = {\n",
        "    'title_list': [title_list],\n",
        "    'author_list': [author_list],\n",
        "    'poem_list': poem_list\n",
        "}\n",
        "#with open('poem_data.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(data, file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –ø–µ—Ä–µ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π\n",
        "with open ('poem_data.json', 'r', encoding='utf-8') as file: #–æ—Ç–∫—Ä—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ä–∞–Ω–µ–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –≤ —Ö–æ–¥–µ –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
        "    datum = json.load(file)\n",
        "lemmas_list = []\n",
        "for i in datum['poem_list']:\n",
        "    doc = re.sub(r'[^–∞-—è—ë–ê-–Ø–Åa-zA-Z]', ' ', i)\n",
        "    doc = nlp(doc)\n",
        "    lemmas = [token.lemma_ for token in doc if not token.is_space]\n",
        "    lemmas_list.append (lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open ('poem_data.json', 'r', encoding='utf-8') as file: #–æ—Ç–∫—Ä—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ä–∞–Ω–µ–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –≤ —Ö–æ–¥–µ –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
        "    datum = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏\n",
        "data = {'lemmas_list': lemmas_list}\n",
        "with open('lemmas_data.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(data, file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open ('lemmas_data.json', 'r', encoding='utf-8') as file: #–æ—Ç–∫—Ä—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ä–∞–Ω–µ–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –≤ —Ö–æ–¥–µ –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
        "    datum2 = json.load(file)\n",
        "lemmas = datum2['lemmas_list']\n",
        "texts = [' '.join(poem) for poem in lemmas]\n",
        "stopwords_list = list (stopwords)\n",
        "#–¥–µ–ª–∏–º —Ç–µ–∫—Å—Ç—ã –Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ö–æ–∂–∏–µ –≥—Ä—É–ø–ø—ã\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "vectorizer = CountVectorizer(max_features=1000, stop_words=stopwords_list)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LDA\n",
        "num_topics = 15\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "lda.fit(X)\n",
        "\n",
        "# –≤—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º –ø–æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è–º\n",
        "doc_topic_dist = lda.transform(X)\n",
        "dominant_topics = np.argmax(doc_topic_dist, axis=1)\n",
        "\n",
        "#–º–æ–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ –∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞–±–æ—Ä—ã –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤ –∫–∞–∂–¥–æ–π —Ç–µ–º–µ —Å–ª–æ–≤\n",
        "#for topic_idx, topic_words in enumerate(lda.components_):\n",
        "#    top_words_idx = topic_words.argsort()[-10:][::-1]\n",
        "#    top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_idx]\n",
        "#    print(f\"–¢–µ–º–∞ {topic_idx + 1}: {', '.join(top_words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#–ø–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤ —Å —Ü–µ–ª—å—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –µ–¥–∏–Ω–∏—Ü\n",
        "texts = [' '.join(poem) for poem in lemmas] #–¥–µ–ª–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–¥–æ–±–æ–≤–∞—Ä–∏–º—ã–º –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä–∞\n",
        "vect = TfidfVectorizer (stop_words=list(stopwords), max_features=32000)\n",
        "dataset = vect.fit_transform(texts) #–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤, –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏\n",
        "idf_values = vect.idf_ \n",
        "feature_names = vect.get_feature_names_out()\n",
        "df_idf = pd.DataFrame({'—Å–ª–æ–≤–æ': feature_names, 'idf': idf_values})\n",
        "df_sorted_asc = df_idf.sort_values('idf', ascending=True)\n",
        "print(df_sorted_asc.head(50)) #–ø—Ä–æ—Å–º–æ—Ç—Ä –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –µ–¥–∏–Ω–∏—Ü - –Ω–µ–∏–Ω–π–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ —Å—Ç–æ–ø –ª–∏—Å—Ç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#–ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å–∫—Ç–≤–∞ —Ç–µ–º –¥–ª—è LDA –ø—Ä–∏ –ø–æ–º–æ—â–∏ —Ä–∞—Å—Å—á—ë—Ç–∞ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ –∏ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏\n",
        "perplexity_scores = []\n",
        "coherence_scores = []\n",
        "min_topics = 2\n",
        "max_topics = 25\n",
        "step = 1\n",
        "topic_range = range(min_topics, max_topics + 1, step)\n",
        "\n",
        "#—Ä–∞—Å—Å—á—ë—Ç –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏\n",
        "texts_for_sklearn = [' '.join(poem) for poem in lemmas]\n",
        "vectorizer = CountVectorizer(max_features=1000, stop_words=stopwords_list) #—Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Å–æ —Å–ª–æ–≤–∞–º–∏ –∏ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—è–º–∏ –∏—Ö —É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è\n",
        "X = vectorizer.fit_transform(texts_for_sklearn)\n",
        "lda_sklearn = LatentDirichletAllocation(n_components=k, #–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LDA\n",
        "                                        random_state=42,\n",
        "                                        max_iter=100,\n",
        "                                        learning_method='online')\n",
        "lda_sklearn.fit(X) #—Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LDA\n",
        "perplexity = lda_sklearn.perplexity(X) #—Ä–∞—Å—Å—á—ë—Ç –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏\n",
        "perplexity_scores.append(perplexity)\n",
        "plot1.plot(topic_range, perplexity_scores, marker='o')\n",
        "plot1.set_xlabel('–¢–µ–º—ã')\n",
        "plot1.set_ylabel('–ü–µ—Ä–ø–ª–µ–∫—Å–∏—è')\n",
        "plot1.grid(True)\n",
        "\n",
        "#—Ä–∞—Å—Å—á—ë—Ç –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏\n",
        "texts_for_gensim = lemmas\n",
        "id2word = dictionary(texts_for_gensim)\n",
        "corpus = [id2word.doc2bow(text) for text in texts_for_gensim]\n",
        "lda_gensim = LdaModel(corpus=corpus,\n",
        "                      id2word=id2word,\n",
        "                      num_topics=k,\n",
        "                      random_state=42,\n",
        "                      passes=10, \n",
        "                      alpha='auto')\n",
        "coherence_model = CoherenceModel(model=lda_gensim, #–Ω—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏\n",
        "                                  texts=texts_for_gensim,\n",
        "                                  dictionary=id2word,\n",
        "                                  coherence='c_v')  \n",
        "coherence = coherence_model.get_coherence() #—Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏\n",
        "coherence_scores.append(coherence)\n",
        "plot2.plot(topic_range, coherence_scores, marker='s', color='orange')\n",
        "plot2.set_xlabel('–¢–µ–º—ã')\n",
        "plot2.set_ylabel('–ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å')\n",
        "plot2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å—ã —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π –∏ –∏—Ö —Ç–µ–º—ã –≤ —Ñ–∞–π–ª\n",
        "with open ('poem_data.json', 'r', encoding='utf-8') as file: #–æ—Ç–∫—Ä—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ä–∞–Ω–µ–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –≤ —Ö–æ–¥–µ –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
        "    datum = json.load(file)\n",
        "poem_indices = list(range(len(datum['poem_list'])))\n",
        "df = pd.DataFrame({\n",
        "    '—Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ': poem_indices,          # –∏–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è\n",
        "    '—Ç–µ–º–∞': dominant_topics\n",
        "})\n",
        "df.to_csv('poem_table.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∏–¥–æ–≤ —Ä–∏—Ç–º–∞ –∏ —Ä–∏—Ñ–º—ã –≤ —Ç–µ–∫—Å—Ç–∞—Ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open ('poem_data.json', 'r', encoding='utf-8') as file: #–æ—Ç–∫—Ä—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ä–∞–Ω–µ–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –≤ —Ö–æ–¥–µ –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
        "    datum = json.load(file)\n",
        "\n",
        "#–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∏–¥–æ–≤ —Ä–∏—Ç–º–∞ –≤ —Ç–µ–∫—Å—Ç–∞—Ö\n",
        "\n",
        "def parting (text): #—Å–æ–∑–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–æ–∫–∏\n",
        "    parted = text.lower()\n",
        "    parted = re.sub(r'\\xa0', ' ', parted) #—É–±–∏—Ä–∞–µ–º –Ω–µ–∫—Ä–∞—Å–∏–≤—É—é —à—Ç—É–∫—É\n",
        "    parted = re.sub(r'\\n', '–†–†–†', parted) #–∏—Å–ø–æ–ª—å–∑—É–µ–º —É—Å–ª–æ–≤–Ω—ã–π –Ω–∞–±–æ—Ä —Å–∏–º–≤–æ–ª–æ–≤, —á—Ç–æ–±—ã —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Ç–µ–∫—Å—Ç—ã\n",
        "    parted = [part.strip() for part in parted.split(\"–†–†–†\") if part.strip()]\n",
        "    return (parted)  \n",
        "\n",
        "def accentizing (text): #—Ä–∞—Å—Å—Ç–∞–≤–ª—è–µ–º —É–¥–∞—Ä–µ–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ö —Å—Ç—Ä–æ–∫\n",
        "    parted = parting (text)\n",
        "    accentized = [accentor(s) for s in parted[:10]]\n",
        "    accentized = [re.sub(r'[–Å—ë]', '—ë\\'', s) for s in accentized] #—Å—Ç–∞–≤–∏–º —É–¥–∞—Ä–µ–Ω–∏—è –Ω–∞–¥ —ë, –ø–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —É–¥–∞—Ä–µ–Ω–∏—è–º–∏\n",
        "    accentized = [encoding_vowels(s) for s in accentized]\n",
        "    return (accentized)\n",
        "\n",
        "def encoding_vowels(accentized): #–¥–µ–ª–∞–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É–¥–∞—Ä–Ω—ã—Ö –∏ –±–µ–∑—É–¥–∞—Ä–Ω—ã—Ö –≥–ª–∞—Å–Ω—ã—Ö, –≥–¥–µ 1 - —É–¥–∞—Ä–Ω—ã–µ, 0 - –±–µ–∑—É–¥–∞—Ä–Ω—ã–µ\n",
        "    encoded = re.sub(r'[^–∞–µ—ë–∏–æ—É—ã—ç—é—è\\']', '', accentized) #–∑–∞–º–µ–Ω—è–µ–º \n",
        "    encoded = re.sub(r'[(–∞–µ—ë–∏–æ—É—ã—ç—é—è)]\\'', '1', encoded)\n",
        "    encoded = re.sub(r'[(–∞–µ—ë–∏–æ—É—ã—ç—é—è)]', '0', encoded)\n",
        "    return (encoded)\n",
        "\n",
        "def rhythm_counter(accentized): #–æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è –∏ —É—Å–ª–æ–≤–Ω–æ –æ–±–æ–∑–Ω–∞—á–∞–µ–º –∫–∞–∂–¥—ã–π —á–∏—Å–ª–æ–º –æ—Ç 0 –¥–æ 4\n",
        "    accentized = accentizing (accentized)\n",
        "    chunks = [[s[i:i+2] for i in range(0, len(s), 2)] for s in accentized] #–¥–µ–ª–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ü–∏—Ñ—Ä –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –¥–≤–µ —Ü–∏—Ñ—Ä—ã –≤ –∫–∞–∂–¥–æ–π, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —è–º–± –∏ —Ö–æ—Ä–µ–π\n",
        "    iambus = 0\n",
        "    choreus = 0\n",
        "    for chunk in chunks:\n",
        "        for element in chunk:\n",
        "            if element == '01':\n",
        "                iambus += 1\n",
        "            elif element == '10':\n",
        "                choreus += 1\n",
        "    num_chunks = sum(len(sublist) for sublist in chunks)\n",
        "    iambus_score = iambus/num_chunks #–Ω–∞—Ö–æ–¥–∏–º –¥–æ–ª—é –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —è–º–±–∞\n",
        "    choreus_score = choreus/num_chunks #–Ω–∞—Ö–æ–¥–∏–º –¥–æ–ª—é –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ —Ö–æ—Ä–µ—è\n",
        "    chunks = [[s[i:i+3] for i in range(0, len(s), 3)] for s in accentized] #–¥–µ–ª–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ü–∏—Ñ—Ä –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Ç—Ä–∏ —Ü–∏—Ñ—Ä—ã –≤ –∫–∞–∂–¥–æ–π, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–∫—Ç–∏–ª—å, –∞–º—Ñ–∏–±—Ä–∞—Ö–∏–π –∏ –∞–Ω–∞–ø–µ—Å—Ç\n",
        "    dactyl = 0\n",
        "    anapest = 0\n",
        "    amphibrachium = 0\n",
        "    for chunk in chunks:\n",
        "        for element in chunk:\n",
        "            if element == '100':\n",
        "                dactyl += 1\n",
        "            elif element == '010':\n",
        "                amphibrachium += 1\n",
        "            elif element == '001': \n",
        "                anapest += 1\n",
        "    num_chunks = sum(len(sublist) for sublist in chunks)\n",
        "    dactyl_score = dactyl/num_chunks #–Ω–∞—Ö–æ–¥–∏–º –¥–æ–ª—é –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –¥–∞–∫—Ç–∏–ª—è\n",
        "    amphibrachium_score = amphibrachium/num_chunks #–Ω–∞—Ö–æ–¥–∏–º –¥–æ–ª—é –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∞–º—Ñ–∏–±—Ä–∞—Ö–∏—è\n",
        "    anapest_score = anapest/num_chunks #–Ω–∞—Ö–æ–¥–∏–º –¥–æ–ª—é –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ø–µ—Å—Ç–∞\n",
        "    score = max (iambus_score, choreus_score, dactyl_score, amphibrachium_score, anapest_score)\n",
        "    scores = [choreus_score, iambus_score, dactyl_score, amphibrachium_score, anapest_score]\n",
        "    score = scores.index(max(scores)) #–≤—ã–≤–æ–¥–∏–º —Ä–∞–∑–º–µ—Ä –≤–≤–∏–¥–µ —É—Å–ª–æ–≤–Ω–æ–≥–æ —á–∏—Å–ª–∞\n",
        "    print (score)\n",
        "    return (score)\n",
        "\n",
        "score_line = [] #—Å–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
        "\n",
        "for i in datum['poem_list']:    \n",
        "    score_line.append(rhythm_counter(i))\n",
        "\n",
        "#–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∏–¥–æ–≤ —Ä–∏—Ñ–º—ã\n",
        "\n",
        "def rhyme_counter (parted):\n",
        "    parted = parting(parted) #–¥–µ–ª–∏–º —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è –Ω–∞ —Å—Ç—Ä–æ–∫–∏\n",
        "    parted = (re.sub(r'[^\\w\\s]', '', i) for i in parted) #—É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ —Å—Ç—Ä–æ–∫\n",
        "    endings = []\n",
        "    endings = [i[-2:] for i in parted] #–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–≤ –≤ —Å—Ç—Ä–æ–∫–∞—Ö (–≤—Ä–æ–¥–µ \"–∞—è\", \"–∏–º\" –∏ —Ç.–¥.)\n",
        "    ends1 = []\n",
        "    ends2 = []\n",
        "    for i in range(0, len(endings), 4): #—Å–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–∫–∏ –æ–∫–æ–Ω—á–∞–Ω–∏–π —Å–æ—Å–µ–¥–Ω–∏—Ö —Å—Ç—Ä–æ–∫ (1 - 2 –∏ —Ç.–¥.)\n",
        "        ends1.extend(endings[i:i+2])\n",
        "        ends2.extend(endings[i+2:i+4])\n",
        "    paired_rhyme = pattern_similarity (ends1, ends2) #—Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –ø–∞—Ä–Ω–æ–π —Ä–∏—Ñ–º—ã\n",
        "    if paired_rhyme == 10: \n",
        "        return (2) #–≤—ã–≤–æ–¥–∏–º –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∑–∞–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
        "    ends1 = endings[0::2] #—Å–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–∫–∏ –æ–∫–æ–Ω—á–∞–Ω–∏–π –≤ —Å—Ç—Ä–æ–∫–∞—Ö —á–µ—Ä–µ–∑ –æ–¥–Ω—É (1 - 3 –∏ —Ç.–¥.)\n",
        "    ends2  = endings[1::2]\n",
        "    in_one_rhyme = pattern_similarity (ends1, ends2) #—Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–æ–π —Ä–∏—Ñ–º—ã\n",
        "    rhyme_score = max (paired_rhyme, in_one_rhyme) \n",
        "    if rhyme_score == paired_rhyme:\n",
        "        rhyme_score = 0\n",
        "    else:\n",
        "        rhyme_score = 1\n",
        "    print (rhyme_score)\n",
        "    return (rhyme_score)\n",
        "\n",
        "def pattern_similarity(ends1, ends2): #—Å–æ–∑–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –≤ –¥–≤—É—Ö —Å–ø–∏—Å–∫–∞—Ö –æ–∫–æ–Ω—á–∞–Ω–∏–π\n",
        "    min_len = min(len(ends1), len(ends2)) #—Å–æ–∫—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–∫–∏ –¥–æ –¥–ª–∏–Ω—ã –∫—Ä–∞—Ç—á–∞–π—à–µ–≥–æ –∏–∑ –Ω–∏—Ö, —á—Ç–æ–±—ã –Ω–µ –≤–æ–∑–Ω–∏–∫–∞–ª–æ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π\n",
        "    if min_len == 0:\n",
        "        return (10) #–¥–ª—è —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏, —Å–æ–∑–¥–∞—ë–º –æ—Å–æ–±–æ–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ. —Ç–∞–∫–∏—Ö —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π –≤ –±–∞–∑–µ –¥–≤–∞, –æ–Ω–∏ –æ–±–∞ –Ω–∞–ø–∏—Å–∞–Ω—ã –≤ –ø—Ä–æ–∑–µ\n",
        "    matches = sum(1 for i in range(min_len) if ends1[i] == ends2[i])\n",
        "    return (matches / min_len)\n",
        "\n",
        "rhyme_line = [] #—Å–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ –≤–∏–¥–æ–≤ —Ä–∏—Ñ–º —Ç–µ–∫—Å—Ç–æ–≤\n",
        "for i in datum['poem_list']:\n",
        "    rhyme_line.append(rhyme_counter(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#—Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä—ã —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π –≤ —Ñ–∞–π–ª\n",
        "df = pd.read_csv('poem_table.csv', encoding='utf-8')\n",
        "df['—Ä–∏—Ç–º'] = score_line\n",
        "df.to_csv('poem_table.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#—Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∏–¥—ã —Ä–∏—Ñ–º —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π –≤ —Ñ–∞–π–ª\n",
        "df = pd.read_csv('poem_table.csv', encoding='utf-8')\n",
        "df['—Ä–∏—Ñ–º–∞'] = rhyme_line\n",
        "df.to_csv('poem_table.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#–¥–æ–±–∞–≤–ª—è–µ–º –≤ —Ç–∞–±–ª–∏—Ü—É —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è—Ö –∏—Ö —Ç–µ–∫—Å—Ç—ã\n",
        "text_list = datum ['poem_list']\n",
        "df = pd.read_csv('poems_table.csv')\n",
        "df['—Ç–µ–∫—Å—Ç'] = text_list\n",
        "df.to_csv('poems_table.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('poem_table.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#—Ä–∞—Å—Å—á—ë—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –º–µ—Ç–æ–¥–æ–º –ª–æ–∫—Ç—è\n",
        "hv = HybridVectorizer( #–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "    column_encodings={\n",
        "        '—Ç–µ–∫—Å—Ç': 'text',\n",
        "        '—Ç–µ–º–∞': 'categorical',\n",
        "        '—Ä–∏—Ç–º': 'categorical',\n",
        "        '—Ä–∏—Ñ–º–∞': 'categorical'\n",
        "    }\n",
        ")\n",
        "vectors = hv.fit_transform(df)\n",
        "\n",
        "inert = []\n",
        "K_range = range(2, 10)  #–∑–∞–¥–∞—ë–º –¥–∏–∞–ø–∞–∑–æ–Ω –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–±—ã–ª–∏ –æ–ø—Ä–æ–±–æ–≤–∞–Ω—ã 2-20 –∏ 2-10)\n",
        "for k in K_range:\n",
        "    print(f\"k={k}...\")\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(vectors)\n",
        "    inert.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8,5)) #–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏ —Å—Ç—Ä–æ–∏ –≥—Ä–∞—Ñ–∏–∫\n",
        "plt.plot(K_range, inert, marker='o')\n",
        "plt.xlabel('–ö–ª–∞—Å—Ç–µ—Ä—ã')\n",
        "plt.ylabel('–ò–Ω–µ—Ä—Ü–∏—è')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-24 11:04:15,839 [INFO] HybridVectorizer - Using CPU for text encoding (GPU not available)\n",
            "2026-02-24 11:04:15,842 [INFO] HybridVectorizer - Initializing HybridVectorizer\n",
            "2026-02-24 11:04:15,845 [INFO] HybridVectorizer - Loading text model: all-MiniLM-L6-v2\n",
            "2026-02-24 11:04:15,855 [INFO] sentence_transformers.SentenceTransformer - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
            "2026-02-24 11:04:17,058 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 11:04:17,088 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:17,257 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 11:04:17,287 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:17,458 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 11:04:17,498 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:17,669 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 11:04:17,704 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:17,882 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 11:04:17,917 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:18,073 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "2026-02-24 11:04:18,078 [WARNING] huggingface_hub.utils._http - Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "2026-02-24 11:04:18,113 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:18,340 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-24 11:04:18,515 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 11:04:18,555 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json \"HTTP/1.1 200 OK\"\n",
            "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 202.13it/s, Materializing param=pooler.dense.weight]                             \n",
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "2026-02-24 11:04:19,546 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 11:04:19,587 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:19,767 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 11:04:19,801 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:19,994 [INFO] httpx - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-24 11:04:20,167 [INFO] httpx - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:20,534 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-24 11:04:20,577 [INFO] httpx - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:21,130 [INFO] httpx - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 \"HTTP/1.1 200 OK\"\n",
            "2026-02-24 11:04:21,151 [INFO] HybridVectorizer - Fitting and transforming input DataFrame\n",
            "2026-02-24 11:04:21,155 [INFO] HybridVectorizer - Starting fit_transform...\n",
            "2026-02-24 11:04:21,162 [INFO] HybridVectorizer - [—Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ] -> numerical (dtype: int64)\n",
            "2026-02-24 11:04:21,181 [INFO] HybridVectorizer - [—Ç–µ–º–∞] Manual override: categorical\n",
            "2026-02-24 11:04:21,183 [INFO] HybridVectorizer - [—Ç–µ–º–∞] -> categorical (manual override, str)\n",
            "2026-02-24 11:04:21,199 [INFO] HybridVectorizer - [—Ä–∏—Ç–º] Manual override: categorical\n",
            "2026-02-24 11:04:21,200 [INFO] HybridVectorizer - [—Ä–∏—Ç–º] -> categorical (manual override, str)\n",
            "2026-02-24 11:04:21,213 [INFO] HybridVectorizer - [—Ä–∏—Ñ–º–∞] Manual override: categorical\n",
            "2026-02-24 11:04:21,217 [INFO] HybridVectorizer - [—Ä–∏—Ñ–º–∞] -> categorical (manual override, str)\n",
            "2026-02-24 11:04:21,228 [INFO] HybridVectorizer - [—Ç–µ–∫—Å—Ç] Manual override: text\n",
            "2026-02-24 11:04:21,231 [INFO] HybridVectorizer - [—Ç–µ–∫—Å—Ç] -> text (manual override, str)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Processing data\n",
            "üìù Encoding text: —Ç–µ–∫—Å—Ç...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-24 11:05:14,342 [INFO] HybridVectorizer - [–∫–ª–∞—Å—Ç–µ—Ä—ã] -> numerical (dtype: int64)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Generated 480 vectors with 395 dimensions\n"
          ]
        }
      ],
      "source": [
        "#–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã\n",
        "hv = HybridVectorizer(\n",
        "    column_encodings={\n",
        "        '—Ç–µ–∫—Å—Ç': 'text',\n",
        "        '—Ç–µ–º–∞': 'categorical',\n",
        "        '—Ä–∏—Ç–º': 'categorical',\n",
        "        '—Ä–∏—Ñ–º–∞': 'categorical'\n",
        "    },\n",
        ")\n",
        "vectors = hv.fit_transform(df)\n",
        "n_clusters = 7\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#—Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–º–µ—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —Ñ–∞–π–ª–µ\n",
        "df['–∫–ª–∞—Å—Ç–µ—Ä—ã'] = kmeans.fit_predict(vectors)\n",
        "df.to_csv('poem_table.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_closest_in_cluster(poem_index): #—Å–æ–∑–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞, –Ω–∞–∏–±–æ–ª–µ–µ —Å—Ö–æ–∂–µ–≥–æ —Å –∏—Å—Ö–æ–¥–Ω—ã–º\n",
        "    cluster = df.loc[poem_index, '–∫–ª–∞—Å—Ç–µ—Ä']\n",
        "    target_vec = vectors[poem_index]\n",
        "    mask = (df['hybrid_cluster'] == cluster) & (df.index != poem_index) #–Ω–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞, –∏—Å–∫–ª—é—á–∞—è –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "    candidate_indices = df.index[mask].tolist()\n",
        "    candidate_vectors = vectors[candidate_indices]\n",
        "    distances = np.linalg.norm(candidate_vectors - target_vec, axis=1) #–≤—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏\n",
        "    best_idx_local = np.argmin(distances) #–Ω–∞—Ö–æ–¥–∏–º –≤–µ–∫—Ç–æ—Ä —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –¥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ\n",
        "    best_idx = candidate_indices[best_idx_local]\n",
        "    return (best_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–ë–æ—Ç –∏ –∞–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤ –ø—Ä–∏ –ø–æ–º–æ—â–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titles = datum['title_list'][0]\n",
        "authors = datum ['author_list'][0]\n",
        "bot = TeleBot(\"XXX\")  #–∑–¥–µ—Å—å —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–æ–¥ –±–æ—Ç–∞\n",
        "\n",
        "with open ('poem_data.json', 'r', encoding='utf-8') as file: #–æ—Ç–∫—Ä—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ä–∞–Ω–µ–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –≤ —Ö–æ–¥–µ –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
        "    datum = json.load(file)\n",
        "\n",
        "@bot.message_handler(commands=['start']) #—Å–æ–∑–¥–∞—ë–º –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º–æ–µ –±–æ—Ç–æ–º –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞\n",
        "def start(message):\n",
        "    bot.send_message(message.from_user.id, \"–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π! –Ø - —Ç–≤–æ–π –ª–∏—á–Ω—ã–π –ø—Ä–æ–≤–æ–¥–Ω–∏–∫ –≤ –º–∏—Ä –ø–æ—ç–∑–∏–∏. \\n\\n–í–≤–µ–¥–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–Ω—Ä–∞–≤–∏–≤—à–µ–≥–æ—Å—è —Ç–µ–±–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è –∏ –ø–æ–ª—É—á–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –ø–æ–¥–±–æ—Ä–∫—É —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —á—Ç–µ–Ω–∏—è.\")\n",
        "    bot.register_next_step_handler(message, get_poem_name)\n",
        "\n",
        "def get_poem_name(message): #–∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π\n",
        "    global poem_name\n",
        "    poem_name = message.text.strip()\n",
        "    bot.send_message(message.from_user.id, '–í–æ—Ç –ø—è—Ç—å —Ç–µ–∫—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è –º–æ–≥—É —Ç–µ–±–µ –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å:')\n",
        "    give_texts (message)\n",
        "\n",
        "def give_texts (message):\n",
        "    x = poem_finder (poem_name, message) #–ø—Ä–∏ –ø–æ–º–æ—â–∏ –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –Ω–∞—Ö–æ–¥–∏–º —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –≤ –∫–æ—Ä–ø—É—Å–µ\n",
        "    y = comparing_texts (x, vectors) #–Ω–∞—Ö–æ–¥–∏–º —Ç–µ–∫—Å—Ç—ã, –Ω–∞–∏–±–æ–ª–µ–µ —Å—Ö–æ–∂–∏–µ —Å –∑–∞–¥–∞–Ω–Ω—ã–º\n",
        "    recommended_author = authors[y]\n",
        "    recommended_title = ' '.join(titles[y])\n",
        "    bot.send_message (\n",
        "        message.chat.id,\n",
        "        f\"–í–æ—Ç —á—Ç–æ —è –º–æ–≥—É –≤–∞–º –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å:\\n\"\n",
        "        f\"\\\"{recommended_title}\\\"\\n{recommended_author}\",\n",
        "        parse_mode='HTML'\n",
        "    ) #–æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "\n",
        "def poem_finder (poem_name, message):\n",
        "        poem_name = set(word_tokenize (poem_name))\n",
        "        c = 0\n",
        "        for i in (datum ['title_list'][0]): #–Ω–∞—Ö–æ–¥–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ, –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–µ–µ –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–µ\n",
        "            m = set (i)\n",
        "            k = jaccard_similarity (poem_name, m)\n",
        "            if k > c:\n",
        "                c = k\n",
        "            else:\n",
        "                number = (datum ['title_list'][0]).index (i)\n",
        "        return (number)\n",
        "        \n",
        "def jaccard_similarity(set_a, set_b): \n",
        "    intersection = len(set_a & set_b)   \n",
        "    union = len(set_a | set_b)          \n",
        "    if union == 0:\n",
        "        return 0                        \n",
        "    return (intersection / union)\n",
        "            \n",
        "def comparing_texts (poem_index, vectors): #–∏—â–µ–º —Ç–µ–∫—Å—Ç, –Ω–∞–∏–±–æ–ª–µ–µ —Å—Ö–æ–∂–µ–≥–æ —Å –∏—Å—Ö–æ–¥–Ω—ã–º\n",
        "    cluster = df.loc[poem_index, '–∫–ª–∞—Å—Ç–µ—Ä—ã']\n",
        "    target_vec = vectors[poem_index]\n",
        "    mask = (df['–∫–ª–∞—Å—Ç–µ—Ä—ã'] == cluster) & (df.index != poem_index) #–Ω–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–π –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞, –∏—Å–∫–ª—é—á–∞—è –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "    candidate_indices = df.index[mask].tolist()\n",
        "    candidate_vectors = vectors[candidate_indices]\n",
        "    distances = np.linalg.norm(candidate_vectors - target_vec, axis=1) #–≤—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏\n",
        "    best_idx_local = np.argmin(distances) #–Ω–∞—Ö–æ–¥–∏–º –≤–µ–∫—Ç–æ—Ä —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –¥–æ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ\n",
        "    best_idx = candidate_indices[best_idx_local]\n",
        "    return (best_idx)\n",
        "\n",
        "#–∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏—ë–º —Å–æ–æ–±—â–µ–Ω–∏–π –±–æ—Ç–æ–º\n",
        "bot.polling(none_stop=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
