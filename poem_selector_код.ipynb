{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#импортируем необходимые библиотеки\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from nltk import word_tokenize\n",
        "import json\n",
        "import telebot\n",
        "import heapq\n",
        "from telebot import TeleBot, types\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('russian')\n",
        "my_stop_words = {'ка', 'весь', 'свой', 'это', 'лишь'}\n",
        "stopwords = my_stop_words.union(stopwords)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from ruaccent import load_accentor\n",
        "accentor = load_accentor()\n",
        "from hybrid_vectorizer import HybridVectorizer\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nlp = spacy.load(\"ru_core_news_md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Парсинг имён авторов, названий и текстов стихотворений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#получаем ссылки на страницы сайта\n",
        "def page_generator (x,y):\n",
        "    page_links = []\n",
        "    for i in range(x, y):\n",
        "        page_links.append(f'https://www.culture.ru/literature/poems?page={i}')\n",
        "    return (page_links)\n",
        "\n",
        "#получаем ссылки на стихотворения\n",
        "def link_generator (page_links):\n",
        "    poem_links = []\n",
        "    poem_elements = []\n",
        "    for i in page_links:\n",
        "        page = requests.get(i)\n",
        "        if page.status_code != 200:\n",
        "            print ('ошибка подключения к странице')\n",
        "            break\n",
        "        time.sleep(3)\n",
        "        soup = BeautifulSoup(page.text)\n",
        "        poem_elements = soup.find_all('a', class_='styles_PoemCard__Heading__SJxDJ')\n",
        "        for element in poem_elements:\n",
        "            href = element.get('href')\n",
        "            full_url = ['https://www.culture.ru' + href]\n",
        "            poem_links.extend(full_url)\n",
        "    return (poem_links)\n",
        "\n",
        "#создаём функцию для поиска текста стихотворения на странице\n",
        "def poem_finder (soup):\n",
        "    poem = soup.find (class_='styles_body__WEo9w')\n",
        "    poem = str(poem)\n",
        "    poem = poem.replace ('<br/>', '\\n')\n",
        "    poem = BeautifulSoup(poem, 'html.parser').text\n",
        "    return (poem)\n",
        "\n",
        "#создаём функцию для поиска имени автора\n",
        "def author_finder (soup):\n",
        "    author = soup.find (class_='styles_MainPoem__Author__SjIRK').text\n",
        "    return (author)\n",
        "\n",
        "#создаём функцию для поиска и токенизации названия стиховторения\n",
        "def title_finder (soup):\n",
        "    title = soup.find (class_='styles_MainPoem__Title__nvD6h').text\n",
        "    title = word_tokenize (title)\n",
        "    return (title)\n",
        "\n",
        "#находим имя автора, название и текст каждого стихотворения\n",
        "author_list = []\n",
        "title_list = []\n",
        "poem_list = []\n",
        "ranges = [(1,5), (5,10), (10,15), (15,21)]\n",
        "for x, y in ranges:\n",
        "    page_links = page_generator (x,y)\n",
        "    print ('ссылки на страницы:', page_links)\n",
        "    poem_links = link_generator (page_links)\n",
        "    print ('ссылки на стихотворения:', poem_links)\n",
        "    time.sleep(3)\n",
        "    for link in poem_links:\n",
        "        page = requests.get(link)\n",
        "        if page.status_code != 200:\n",
        "            print ('ошибка подключения к странице')\n",
        "            page = requests.get(link)\n",
        "            if page.status_code != 200:\n",
        "                print ('повторная ошибка подключения к странице')\n",
        "                continue\n",
        "        time.sleep(3)\n",
        "        soup = BeautifulSoup(page.text)\n",
        "        author = author_finder (soup)\n",
        "        title = title_finder (soup)\n",
        "        poem = poem_finder (soup)\n",
        "        author_list.append (author)\n",
        "        title_list.append (title)\n",
        "        poem_list.append (poem)\n",
        "        print (title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#сохраняем информацию в файле\n",
        "#data = {\n",
        "    'title_list': [title_list],\n",
        "    'author_list': [author_list],\n",
        "    'poem_list': poem_list\n",
        "}\n",
        "#with open('poem_data.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(data, file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Обработка текстов перед кластеризацией"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Тематическое распределение текстов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#лемматизация текстов стихотворений\n",
        "with open ('poem_data.json', 'r', encoding='utf-8') as file: #открываем созданный ранее файл с данными, полученными в ходе парсинга\n",
        "    datum = json.load(file)\n",
        "lemmas_list = []\n",
        "for i in datum['poem_list']:\n",
        "    doc = re.sub(r'[^а-яёА-ЯЁa-zA-Z]', ' ', i)\n",
        "    doc = nlp(doc)\n",
        "    lemmas = [token.lemma_ for token in doc if not token.is_space]\n",
        "    lemmas_list.append (lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open ('poem_data.json', 'r', encoding='utf-8') as file: #открываем созданный ранее файл с данными, полученными в ходе парсинга\n",
        "    datum = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#сохранение файла с лемматизированными текстами\n",
        "data = {'lemmas_list': lemmas_list}\n",
        "with open('lemmas_data.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(data, file, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open ('lemmas_data.json', 'r', encoding='utf-8') as file: #открываем созданный ранее файл с данными, полученными в ходе парсинга\n",
        "    datum2 = json.load(file)\n",
        "lemmas = datum2['lemmas_list']\n",
        "texts = [' '.join(poem) for poem in lemmas]\n",
        "stopwords_list = list (stopwords)\n",
        "#делим тексты на тематически схожие группы\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# подготовка данных\n",
        "vectorizer = CountVectorizer(max_features=1000, stop_words=stopwords_list)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# применение LDA\n",
        "num_topics = 15\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "lda.fit(X)\n",
        "\n",
        "# вычисляем распределение тем по стихотворениям\n",
        "doc_topic_dist = lda.transform(X)\n",
        "dominant_topics = np.argmax(doc_topic_dist, axis=1)\n",
        "\n",
        "#можно вывести и посмотреть наборы популярных в каждой теме слов\n",
        "#for topic_idx, topic_words in enumerate(lda.components_):\n",
        "#    top_words_idx = topic_words.argsort()[-10:][::-1]\n",
        "#    top_words = [vectorizer.get_feature_names_out()[i] for i in top_words_idx]\n",
        "#    print(f\"Тема {topic_idx + 1}: {', '.join(top_words)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#поиск наиболее часто встречающихся слов с целью фильтрации неинформативных единиц\n",
        "texts = [' '.join(poem) for poem in lemmas] #делаем список удобоваримым для векторайзера\n",
        "vect = TfidfVectorizer (stop_words=list(stopwords), max_features=32000)\n",
        "dataset = vect.fit_transform(texts) #векторизация слов, вычисление частотности\n",
        "idf_values = vect.idf_ \n",
        "feature_names = vect.get_feature_names_out()\n",
        "df_idf = pd.DataFrame({'слово': feature_names, 'idf': idf_values})\n",
        "df_sorted_asc = df_idf.sort_values('idf', ascending=True)\n",
        "print(df_sorted_asc.head(50)) #просмотр наиболее популярных единиц - неинйормативные были добавлены в стоп лист"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#подбор оптимального колическтва тем для LDA при помощи рассчёта перплексии и когерентности\n",
        "perplexity_scores = []\n",
        "coherence_scores = []\n",
        "min_topics = 2\n",
        "max_topics = 25\n",
        "step = 1\n",
        "topic_range = range(min_topics, max_topics + 1, step)\n",
        "\n",
        "#рассчёт перплексии\n",
        "texts_for_sklearn = [' '.join(poem) for poem in lemmas]\n",
        "vectorizer = CountVectorizer(max_features=1000, stop_words=stopwords_list) #создание матрицы со словами и частотностями их употребления\n",
        "X = vectorizer.fit_transform(texts_for_sklearn)\n",
        "lda_sklearn = LatentDirichletAllocation(n_components=k, #настройка LDA\n",
        "                                        random_state=42,\n",
        "                                        max_iter=100,\n",
        "                                        learning_method='online')\n",
        "lda_sklearn.fit(X) #создание модели LDA\n",
        "perplexity = lda_sklearn.perplexity(X) #рассчёт перплексии\n",
        "perplexity_scores.append(perplexity)\n",
        "plot1.plot(topic_range, perplexity_scores, marker='o')\n",
        "plot1.set_xlabel('Темы')\n",
        "plot1.set_ylabel('Перплексия')\n",
        "plot1.grid(True)\n",
        "\n",
        "#рассчёт когерентности\n",
        "texts_for_gensim = lemmas\n",
        "id2word = dictionary(texts_for_gensim)\n",
        "corpus = [id2word.doc2bow(text) for text in texts_for_gensim]\n",
        "lda_gensim = LdaModel(corpus=corpus,\n",
        "                      id2word=id2word,\n",
        "                      num_topics=k,\n",
        "                      random_state=42,\n",
        "                      passes=10, \n",
        "                      alpha='auto')\n",
        "coherence_model = CoherenceModel(model=lda_gensim, #нстройка модели когерентности\n",
        "                                  texts=texts_for_gensim,\n",
        "                                  dictionary=id2word,\n",
        "                                  coherence='c_v')  \n",
        "coherence = coherence_model.get_coherence() #создание модели когерентности\n",
        "coherence_scores.append(coherence)\n",
        "plot2.plot(topic_range, coherence_scores, marker='s', color='orange')\n",
        "plot2.set_xlabel('Темы')\n",
        "plot2.set_ylabel('Когерентность')\n",
        "plot2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#сохраняем индексы стихотворений и их темы в файл\n",
        "with open ('poem_data.json', 'r', encoding='utf-8') as file: #открываем созданный ранее файл с данными, полученными в ходе парсинга\n",
        "    datum = json.load(file)\n",
        "poem_indices = list(range(len(datum['poem_list'])))\n",
        "df = pd.DataFrame({\n",
        "    'стихотворение': poem_indices,          # или реальные названия\n",
        "    'тема': dominant_topics\n",
        "})\n",
        "df.to_csv('poem_table.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Определение видов ритма и рифмы в текстах"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open ('poem_data.json', 'r', encoding='utf-8') as file: #открываем созданный ранее файл с данными, полученными в ходе парсинга\n",
        "    datum = json.load(file)\n",
        "\n",
        "#определение видов ритма в текстах\n",
        "\n",
        "def parting (text): #создаём функцию для разбиения текстов стихотворений на строки\n",
        "    parted = text.lower()\n",
        "    parted = re.sub(r'\\xa0', ' ', parted) #убираем некрасивую штуку\n",
        "    parted = re.sub(r'\\n', 'РРР', parted) #используем условный набор символов, чтобы разделить тексты\n",
        "    parted = [part.strip() for part in parted.split(\"РРР\") if part.strip()]\n",
        "    return (parted)  \n",
        "\n",
        "def accentizing (text): #расставляем ударения в словах строк\n",
        "    parted = parting (text)\n",
        "    accentized = [accentor(s) for s in parted[:10]]\n",
        "    accentized = [re.sub(r'[Ёё]', 'ё\\'', s) for s in accentized] #ставим ударения над ё, получаем текст с полностью проставленными ударениями\n",
        "    accentized = [encoding_vowels(s) for s in accentized]\n",
        "    return (accentized)\n",
        "\n",
        "def encoding_vowels(accentized): #делаем из текстовых строк последовательности ударных и безударных гласных, где 1 - ударные, 0 - безударные\n",
        "    encoded = re.sub(r'[^аеёиоуыэюя\\']', '', accentized) #заменяем \n",
        "    encoded = re.sub(r'[(аеёиоуыэюя)]\\'', '1', encoded)\n",
        "    encoded = re.sub(r'[(аеёиоуыэюя)]', '0', encoded)\n",
        "    return (encoded)\n",
        "\n",
        "def rhythm_counter(accentized): #определяем размер стихотворения и условно обозначаем каждый числом от 0 до 4\n",
        "    accentized = accentizing (accentized)\n",
        "    chunks = [[s[i:i+2] for i in range(0, len(s), 2)] for s in accentized] #делим последовательности цифр на последовательности по две цифры в каждой, чтобы проверить ямб и хорей\n",
        "    iambus = 0\n",
        "    choreus = 0\n",
        "    for chunk in chunks:\n",
        "        for element in chunk:\n",
        "            if element == '01':\n",
        "                iambus += 1\n",
        "            elif element == '10':\n",
        "                choreus += 1\n",
        "    num_chunks = sum(len(sublist) for sublist in chunks)\n",
        "    iambus_score = iambus/num_chunks #находим долю популярности ямба\n",
        "    choreus_score = choreus/num_chunks #находим долю популярности хорея\n",
        "    chunks = [[s[i:i+3] for i in range(0, len(s), 3)] for s in accentized] #делим последовательности цифр на последовательности по три цифры в каждой, чтобы проверить дактиль, амфибрахий и анапест\n",
        "    dactyl = 0\n",
        "    anapest = 0\n",
        "    amphibrachium = 0\n",
        "    for chunk in chunks:\n",
        "        for element in chunk:\n",
        "            if element == '100':\n",
        "                dactyl += 1\n",
        "            elif element == '010':\n",
        "                amphibrachium += 1\n",
        "            elif element == '001': \n",
        "                anapest += 1\n",
        "    num_chunks = sum(len(sublist) for sublist in chunks)\n",
        "    dactyl_score = dactyl/num_chunks #находим долю популярности дактиля\n",
        "    amphibrachium_score = amphibrachium/num_chunks #находим долю популярности амфибрахия\n",
        "    anapest_score = anapest/num_chunks #находим долю популярности анапеста\n",
        "    score = max (iambus_score, choreus_score, dactyl_score, amphibrachium_score, anapest_score)\n",
        "    scores = [choreus_score, iambus_score, dactyl_score, amphibrachium_score, anapest_score]\n",
        "    score = scores.index(max(scores)) #выводим размер ввиде условного числа\n",
        "    print (score)\n",
        "    return (score)\n",
        "\n",
        "score_line = [] #создаём список размеров всех текстов\n",
        "\n",
        "for i in datum['poem_list']:    \n",
        "    score_line.append(rhythm_counter(i))\n",
        "\n",
        "#определение видов рифмы\n",
        "\n",
        "def rhyme_counter (parted):\n",
        "    parted = parting(parted) #делим стихотворения на строки\n",
        "    parted = (re.sub(r'[^\\w\\s]', '', i) for i in parted) #убираем лишние символы из строк\n",
        "    endings = []\n",
        "    endings = [i[-2:] for i in parted] #оставляем только окончания последний слов в строках (вроде \"ая\", \"им\" и т.д.)\n",
        "    ends1 = []\n",
        "    ends2 = []\n",
        "    for i in range(0, len(endings), 4): #создаём списки окончаний соседних строк (1 - 2 и т.д.)\n",
        "        ends1.extend(endings[i:i+2])\n",
        "        ends2.extend(endings[i+2:i+4])\n",
        "    paired_rhyme = pattern_similarity (ends1, ends2) #рассчитываем популярность парной рифмы\n",
        "    if paired_rhyme == 10: \n",
        "        return (2) #выводим обозначение для прозаических текстов\n",
        "    ends1 = endings[0::2] #создаём списки окончаний в строках через одну (1 - 3 и т.д.)\n",
        "    ends2  = endings[1::2]\n",
        "    in_one_rhyme = pattern_similarity (ends1, ends2) #рассчитываем популярность перекрёстной рифмы\n",
        "    rhyme_score = max (paired_rhyme, in_one_rhyme) \n",
        "    if rhyme_score == paired_rhyme:\n",
        "        rhyme_score = 0\n",
        "    else:\n",
        "        rhyme_score = 1\n",
        "    print (rhyme_score)\n",
        "    return (rhyme_score)\n",
        "\n",
        "def pattern_similarity(ends1, ends2): #создаём функцию для поиска соответствий в двух списках окончаний\n",
        "    min_len = min(len(ends1), len(ends2)) #сокращаем списки до длины кратчайшего из них, чтобы не возникало несоответствий\n",
        "    if min_len == 0:\n",
        "        return (10) #для стихотворений, состоящих из одной строки, создаём особое обозначение. таких стихотворений в базе два, они оба написаны в прозе\n",
        "    matches = sum(1 for i in range(min_len) if ends1[i] == ends2[i])\n",
        "    return (matches / min_len)\n",
        "\n",
        "rhyme_line = [] #создаём список видов рифм текстов\n",
        "for i in datum['poem_list']:\n",
        "    rhyme_line.append(rhyme_counter(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#сохраняем размеры стихотворений в файл\n",
        "df = pd.read_csv('poem_table.csv', encoding='utf-8')\n",
        "df['ритм'] = score_line\n",
        "df.to_csv('poem_table.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#сохраняем виды рифм стихотворений в файл\n",
        "df = pd.read_csv('poem_table.csv', encoding='utf-8')\n",
        "df['рифма'] = rhyme_line\n",
        "df.to_csv('poem_table.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Кластеризация текстов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#добавляем в таблицу с информацией о стихотворениях их тексты\n",
        "text_list = datum ['poem_list']\n",
        "df = pd.read_csv('poems_table.csv')\n",
        "df['текст'] = text_list\n",
        "df.to_csv('poems_table.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('poem_table.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#рассчёт оптимального количества кластеров методом локтя\n",
        "hv = HybridVectorizer( #настройка модели\n",
        "    column_encodings={\n",
        "        'текст': 'text',\n",
        "        'тема': 'categorical',\n",
        "        'ритм': 'categorical',\n",
        "        'рифма': 'categorical'\n",
        "    }\n",
        ")\n",
        "vectors = hv.fit_transform(df)\n",
        "\n",
        "inert = []\n",
        "K_range = range(2, 10)  #задаём диапазон количества кластеров (были опробованы 2-20 и 2-10)\n",
        "for k in K_range:\n",
        "    print(f\"k={k}...\")\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(vectors)\n",
        "    inert.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8,5)) #настраиваем и строи график\n",
        "plt.plot(K_range, inert, marker='o')\n",
        "plt.xlabel('Кластеры')\n",
        "plt.ylabel('Инерция')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#кластеризируем тексты\n",
        "hv = HybridVectorizer(\n",
        "    column_encodings={\n",
        "        'текст': 'text',\n",
        "        'тема': 'categorical',\n",
        "        'ритм': 'categorical',\n",
        "        'рифма': 'categorical'\n",
        "    },\n",
        ")\n",
        "vectors = hv.fit_transform(df)\n",
        "n_clusters = 7\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#сохраняем номера кластеров текстов в файле\n",
        "df['кластеры'] = kmeans.fit_predict(vectors)\n",
        "df.to_csv('poem_table.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_closest_in_cluster(poem_index): #создаём функцию для поиска текста, наиболее схожего с исходным\n",
        "    cluster = df.loc[poem_index, 'кластер']\n",
        "    target_vec = vectors[poem_index]\n",
        "    mask = (df['hybrid_cluster'] == cluster) & (df.index != poem_index) #находим индексы стихотворений одного кластера, исключая исходный текст\n",
        "    candidate_indices = df.index[mask].tolist()\n",
        "    candidate_vectors = vectors[candidate_indices]\n",
        "    distances = np.linalg.norm(candidate_vectors - target_vec, axis=1) #вычисляем расстояния между векторами\n",
        "    best_idx_local = np.argmin(distances) #находим вектор с наименьшим расстоянием до исходного\n",
        "    best_idx = candidate_indices[best_idx_local]\n",
        "    return (best_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Запуск бота"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#кластеризируем тексты. раскомментируйте, если хотите запустить бот без запуска остального кода\n",
        "df = pd.read_csv('poem_table.csv')\n",
        "\n",
        "hv = HybridVectorizer(\n",
        "    column_encodings={\n",
        "        'текст': 'text',\n",
        "        'тема': 'categorical',\n",
        "        'ритм': 'categorical',\n",
        "        'рифма': 'categorical'\n",
        "    },\n",
        ")\n",
        "vectors = hv.fit_transform(df)\n",
        "n_clusters = 7\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "\n",
        "with open ('poem_data.json', 'r', encoding='utf-8') as file: #открываем созданный ранее файл с данными, полученными в ходе парсинга\n",
        "    datum = json.load(file)\n",
        "titles = datum['title_list'][0]\n",
        "authors = datum ['author_list'][0]\n",
        "bot = TeleBot(\"XXX\")  #здесь содержится уникальный код бота\n",
        "\n",
        "@bot.message_handler(commands=['start']) #создаём первое сообщение, отправляемое ботом после запуска\n",
        "def start(message):\n",
        "    bot.send_message(message.from_user.id, \"Здравствуй! Я - твой личный проводник в мир поэзии. \\n\\nВведи название последнего понравившегося тебе стихотворения и получи персональную подборку текстов для чтения.\")\n",
        "    bot.register_next_step_handler(message, get_poem_name)\n",
        "\n",
        "def get_poem_name(message): #создаём функцию, запускающую процесс поиска стихотворений\n",
        "    global poem_name\n",
        "    poem_name = message.text.strip()\n",
        "    bot.send_message(message.from_user.id, 'Сравниваю тексты...')\n",
        "    give_texts (message)\n",
        "\n",
        "def give_texts (message):\n",
        "    x = poem_finder (poem_name, message) #при помощи полученного названия находим стихотворение в корпусе\n",
        "    y = comparing_texts (x) #находим тексты, наиболее схожие с заданным\n",
        "    recommended_author = authors[y]\n",
        "    recommended_title = ' '.join(titles[y])\n",
        "    bot.send_message (\n",
        "        message.chat.id,\n",
        "        f\"Вот что я могу вам порекомендовать:\\n\"\n",
        "        f\"\\\"{recommended_title}\\\"\\n{recommended_author}\",\n",
        "        parse_mode='HTML'\n",
        "    ) #отправляем пользователю найденный текст\n",
        "\n",
        "def poem_finder (poem_name, message):\n",
        "        poem_name = set(word_tokenize (poem_name))\n",
        "        c = 0\n",
        "        for idx, i in enumerate(datum['title_list'][0]): #находим название, наиболее похожее на заданное\n",
        "            m = set (i)\n",
        "            k = jaccard_similarity (poem_name, m)\n",
        "            if k > c:\n",
        "                c = k\n",
        "                best_i = idx\n",
        "        if c == 0:\n",
        "            bot.send_message(message.from_user.id, 'Этого стихотворения пока нет в моей коллекции :( \\nПопробуй другое название')\n",
        "            return (None)\n",
        "        else:\n",
        "            return (best_i)\n",
        "        \n",
        "def jaccard_similarity(set_a, set_b): \n",
        "    intersection = len(set_a & set_b)   \n",
        "    union = len(set_a | set_b)          \n",
        "    if union == 0:\n",
        "        return 0                        \n",
        "    return (intersection / union)\n",
        "\n",
        "def comparing_texts (poem_index): #ищем текст, наиболее схожего с исходным\n",
        "    cluster = df.loc[poem_index, 'кластеры']\n",
        "    target_vec = vectors[poem_index]\n",
        "    mask = (df['кластеры'] == cluster) & (df.index != poem_index) #находим индексы стихотворений одного кластера, исключая исходный текст\n",
        "    candidate_indices = df.index[mask].tolist()\n",
        "    candidate_vectors = vectors[candidate_indices]\n",
        "    distances = np.linalg.norm(candidate_vectors - target_vec, axis=1) #вычисляем расстояния между векторами\n",
        "    best_idx_local = np.argmin(distances) #находим вектор с наименьшим расстоянием до исходного\n",
        "    best_idx = candidate_indices[best_idx_local]\n",
        "    return (best_idx)\n",
        "\n",
        "#запускаем приём сообщений ботом\n",
        "bot.polling(none_stop=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
